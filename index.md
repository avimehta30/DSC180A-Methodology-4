# Avi Mehta  
**Email:** avmehta@ucsd.edu  

**Section:** A06  
**Mentor:** Hao Zhang  

---

**1. What is the most interesting topic covered in your domain this quarter?**  
One of the most interesting topics we covered in my domain this quarter was trying to find the correct scaling law for our LLM. This was fascinating because I never realized how important scaling laws are until we began training a large scale model for production. When training an LLM with a B200 GPU, you need to ensure that your model scales efficiently to avoid wasted compute and unstable performance.

---

**2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.**  
I want to work on a project paired with our TA, Amitash, on Meta’s CHAI, which focuses on clustering Meta’s multi head attention efficiently. I believe I can contribute meaningfully to this because of everything I learned from building an LLM on a limited budget during Quarter 1.

---

**3. What is a potential change you’d make to the approach taken in your current Quarter 1 Project?**  
A potential change I would make is to use NanoGPT earlier and communicate better with our systems team. I feel that if we had stronger collaboration and earlier integration, our project would have progressed more smoothly and finished much faster.

---

**4. What other techniques would you be interested in using in your project?**  
Other techniques I’d be interested in exploring include testing different hyperparameters and architectures. For example, implementing FlashAttention-3 could significantly reduce memory overhead and improve perplexity performance.

